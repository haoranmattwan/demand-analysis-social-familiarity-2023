{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7da259bd",
   "metadata": {},
   "source": [
    "# Social Familiarity and Reinforcement Value\n",
    "This notebook replicates the analyses for the research on social familiarity and reward value in rats. The original analyses were conducted in R, and this document translates them into a Python workflow.\n",
    "\n",
    "Two distinct analytical approaches are presented:\n",
    "\n",
    "1.  A **frequentist, individual-level analysis** using `lmfit` to fit the Zero-Bounded Exponential (ZBEn) demand model for each subject and condition.\n",
    "2.  A **Bayesian multilevel analysis** using `PyMC` to fit a custom nonlinear model, accounting for the repeated-measures data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa33070f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas numpy scipy lmfit PyMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e7bc8ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lmfit import Model, Parameters\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import pytensor.tensor as pt\n",
    "\n",
    "# Set the display format for floating-point numbers to 3 decimal places\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "\n",
    "# --- Helper Functions (translated from R) ---\n",
    "def lhs(x):\n",
    "    \"\"\"Inverse Hyperbolic Sine (log-like) transformation.\"\"\"\n",
    "    return np.log10(0.5 * x + np.sqrt(0.25 * (x**2) + 1))\n",
    "\n",
    "# --- Load and Process Data ---\n",
    "raw_dat = pd.read_csv(\"Code/dat.csv\")\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "dat = raw_dat.copy()\n",
    "dat['active_ses_time'] = (dat['active_ses_time'] / 60) / 60\n",
    "dat['interact_rt'] = dat['interact'] / dat['active_ses_time']\n",
    "dat['lq'] = lhs(dat['interact_rt'])\n",
    "dat['fmlr'] = np.where(dat['familiarity'] == 1, \"Cagemate\", \"Non-cagemate\")\n",
    "dat['cond'] = dat['cond'].replace({\"10sec\": \"10 Sec\", \"30sec\": \"30 Sec\", \"60sec\": \"60 Sec\"})\n",
    "\n",
    "dat_agg = dat.groupby(['pair', 'fmlr', 'cond', 'fr'], as_index=False).agg(\n",
    "    interact_rt=('interact_rt', 'mean'),\n",
    "    lq=('lq', 'mean')\n",
    ")\n",
    "\n",
    "# Create the specific interaction-like dummy variables required by the models\n",
    "dat_processed = dat_agg.copy()\n",
    "dat_processed['f1'] = ((dat_processed['fmlr'] == 'Cagemate') & (dat_processed['cond'] == '10 Sec')).astype(int)\n",
    "dat_processed['f3'] = ((dat_processed['fmlr'] == 'Cagemate') & (dat_processed['cond'] == '30 Sec')).astype(int)\n",
    "dat_processed['f6'] = ((dat_processed['fmlr'] == 'Cagemate') & (dat_processed['cond'] == '60 Sec')).astype(int)\n",
    "dat_processed['u1'] = ((dat_processed['fmlr'] == 'Non-cagemate') & (dat_processed['cond'] == '10 Sec')).astype(int)\n",
    "dat_processed['u3'] = ((dat_processed['fmlr'] == 'Non-cagemate') & (dat_processed['cond'] == '30 Sec')).astype(int)\n",
    "dat_processed['u6'] = ((dat_processed['fmlr'] == 'Non-cagemate') & (dat_processed['cond'] == '60 Sec')).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241c8212",
   "metadata": {},
   "source": [
    "## Part 1: Frequentist Individual-Level Analysis\n",
    "\n",
    "This section fits the ZBEn demand model to each subject's data and performs linear contrasts on the estimated parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "675e9480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Estimated ZBEn Parameters (Individual Level) ---\n",
      "    pair   Familiarity Social Duration parameter  estimate\n",
      "0      1      Cagemate          10 Sec     alpha    -4.448\n",
      "1      1      Cagemate          30 Sec     alpha    -5.357\n",
      "2      1      Cagemate          60 Sec     alpha    -5.091\n",
      "3      1  Non-cagemate          10 Sec     alpha    -5.869\n",
      "4      1  Non-cagemate          30 Sec     alpha    -4.194\n",
      "5      1  Non-cagemate          60 Sec     alpha    -5.133\n",
      "6      1      Cagemate          10 Sec        Q0    47.955\n",
      "7      1      Cagemate          30 Sec        Q0    55.097\n",
      "8      1      Cagemate          60 Sec        Q0    57.832\n",
      "9      1  Non-cagemate          10 Sec        Q0    46.337\n",
      "10     1  Non-cagemate          30 Sec        Q0    53.984\n",
      "11     1  Non-cagemate          60 Sec        Q0    35.177\n",
      "12     2      Cagemate          10 Sec     alpha    -6.058\n",
      "13     2      Cagemate          30 Sec     alpha    -5.722\n",
      "14     2      Cagemate          60 Sec     alpha    -6.287\n",
      "15     2  Non-cagemate          10 Sec     alpha    -6.261\n",
      "16     2  Non-cagemate          30 Sec     alpha    -5.864\n",
      "17     2  Non-cagemate          60 Sec     alpha    -6.285\n",
      "18     2      Cagemate          10 Sec        Q0    68.034\n",
      "19     2      Cagemate          30 Sec        Q0    94.049\n",
      "20     2      Cagemate          60 Sec        Q0    82.281\n",
      "21     2  Non-cagemate          10 Sec        Q0    75.026\n",
      "22     2  Non-cagemate          30 Sec        Q0    65.055\n",
      "23     2  Non-cagemate          60 Sec        Q0    79.154\n",
      "24     3      Cagemate          10 Sec     alpha    -5.793\n",
      "25     3      Cagemate          30 Sec     alpha    -5.979\n",
      "26     3      Cagemate          60 Sec     alpha    -5.753\n",
      "27     3  Non-cagemate          10 Sec     alpha    -7.181\n",
      "28     3  Non-cagemate          30 Sec     alpha    -6.552\n",
      "29     3  Non-cagemate          60 Sec     alpha    -6.901\n",
      "30     3      Cagemate          10 Sec        Q0    51.310\n",
      "31     3      Cagemate          30 Sec        Q0   126.635\n",
      "32     3      Cagemate          60 Sec        Q0    28.950\n",
      "33     3  Non-cagemate          10 Sec        Q0    93.272\n",
      "34     3  Non-cagemate          30 Sec        Q0    47.529\n",
      "35     3  Non-cagemate          60 Sec        Q0   144.506\n",
      "36     4      Cagemate          10 Sec     alpha    -6.639\n",
      "37     4      Cagemate          30 Sec     alpha    -6.784\n",
      "38     4      Cagemate          60 Sec     alpha    -6.072\n",
      "39     4  Non-cagemate          10 Sec     alpha    -7.404\n",
      "40     4  Non-cagemate          30 Sec     alpha    -6.451\n",
      "41     4  Non-cagemate          60 Sec     alpha    -6.699\n",
      "42     4      Cagemate          10 Sec        Q0    39.086\n",
      "43     4      Cagemate          30 Sec        Q0    25.022\n",
      "44     4      Cagemate          60 Sec        Q0    18.007\n",
      "45     4  Non-cagemate          10 Sec        Q0    73.797\n",
      "46     4  Non-cagemate          30 Sec        Q0    58.536\n",
      "47     4  Non-cagemate          60 Sec        Q0    63.731\n",
      "\n",
      "--- Linear Contrast Results (p-values) ---\n",
      "   pair parameter                   contrast             estimate\n",
      "0     1     alpha  Cagemate vs. Non-cagemate  est=0.000, se=0.000\n",
      "1     1        Q0  Cagemate vs. Non-cagemate  est=0.005, se=0.009\n",
      "2     2     alpha  Cagemate vs. Non-cagemate  est=0.000, se=0.000\n",
      "3     2        Q0  Cagemate vs. Non-cagemate  est=0.001, se=0.001\n",
      "4     3     alpha  Cagemate vs. Non-cagemate  est=0.000, se=0.000\n",
      "5     3        Q0  Cagemate vs. Non-cagemate  est=0.001, se=0.002\n",
      "6     4     alpha  Cagemate vs. Non-cagemate  est=0.000, se=0.000\n",
      "7     4        Q0  Cagemate vs. Non-cagemate  est=0.000, se=0.001\n"
     ]
    }
   ],
   "source": [
    "# --- Define the complex ZBEn model for lmfit ---\n",
    "def zbe_model_full(fr, f1, f3, f6, u1, u3, u6, \n",
    "                   af1, af3, af6, au1, au3, au6, \n",
    "                   qf1, qf3, qf6, qu1, qu3, qu6):\n",
    "    \n",
    "    alpha_term = np.exp(af1*f1 + af3*f3 + af6*f6 + au1*u1 + au3*u3 + au6*u6)\n",
    "    q0_term = qf1*f1 + qf3*f3 + qf6*f6 + qu1*u1 + qu3*u3 + qu6*u6\n",
    "    lhs_q0 = lhs(q0_term)\n",
    "    return lhs_q0 * np.exp((-alpha_term / lhs_q0) * q0_term * fr)\n",
    "\n",
    "zbe_lmfit_model = Model(zbe_model_full, independent_vars=['fr', 'f1', 'f3', 'f6', 'u1', 'u3', 'u6'])\n",
    "\n",
    "# --- Fit the model for each subject ---\n",
    "param_list = []\n",
    "contrast_list = []\n",
    "for subj_pair in dat_processed['pair'].unique():\n",
    "    subj_data = dat_processed[dat_processed['pair'] == subj_pair]\n",
    "    params = zbe_lmfit_model.make_params(af1=-6, af3=-6, af6=-6, au1=-6, au3=-6, au6=-6,\n",
    "                                         qf1=50, qf3=50, qf6=50, qu1=50, qu3=50, qu6=50)\n",
    "    for p in params:\n",
    "        if p.startswith('q'):\n",
    "            params[p].set(min=0)\n",
    "            \n",
    "    independent_vars_dict = {\n",
    "        'fr': subj_data['fr'], 'f1': subj_data['f1'], 'f3': subj_data['f3'],\n",
    "        'f6': subj_data['f6'], 'u1': subj_data['u1'], 'u3': subj_data['u3'],\n",
    "        'u6': subj_data['u6']\n",
    "    }\n",
    "    result = zbe_lmfit_model.fit(subj_data['lq'], params, **independent_vars_dict)\n",
    "    \n",
    "    fit_params = pd.DataFrame.from_dict(result.params.valuesdict(), orient='index', columns=['estimate'])\n",
    "    fit_params['pair'] = subj_pair\n",
    "    param_list.append(fit_params)\n",
    "\n",
    "    alpha_fam_contrast = result.eval_uncertainty(sigma=1, af1=1, af3=1, af6=1, au1=-1, au3=-1, au6=-1)\n",
    "    q0_fam_contrast = result.eval_uncertainty(sigma=1, qf1=1, qf3=1, qf6=1, qu1=-1, qu3=-1, qu6=-1)\n",
    "    \n",
    "    contrast_list.append({'pair': subj_pair, 'parameter': 'alpha', 'contrast': 'Cagemate vs. Non-cagemate', 'estimate': alpha_fam_contrast})\n",
    "    contrast_list.append({'pair': subj_pair, 'parameter': 'Q0', 'contrast': 'Cagemate vs. Non-cagemate', 'estimate': q0_fam_contrast})\n",
    "\n",
    "# --- Reformat and Display Results ---\n",
    "final_params = pd.concat(param_list).reset_index().rename(columns={'index': 'term'})\n",
    "final_params['parameter'] = np.where(final_params['term'].str.startswith('a'), 'alpha', 'Q0')\n",
    "final_params['Familiarity'] = np.where(final_params['term'].str.contains('f'), 'Cagemate', 'Non-cagemate')\n",
    "\n",
    "# Adding a string default value\n",
    "final_params['Social Duration'] = np.select(\n",
    "    [final_params['term'].str.endswith('1'), final_params['term'].str.endswith('3'), final_params['term'].str.endswith('6')],\n",
    "    ['10 Sec', '30 Sec', '60 Sec'],\n",
    "    default='N/A' \n",
    ")\n",
    "\n",
    "final_params = final_params[['pair', 'Familiarity', 'Social Duration', 'parameter', 'estimate']]\n",
    "final_contrasts = pd.DataFrame(contrast_list)\n",
    "print(\"--- Estimated ZBEn Parameters (Individual Level) ---\")\n",
    "print(final_params.to_string())\n",
    "print(\"\\n--- Linear Contrast Results (p-values) ---\")\n",
    "final_contrasts_to_print = final_contrasts.copy()\n",
    "final_contrasts_to_print['estimate'] = final_contrasts_to_print['estimate'].apply(\n",
    "    lambda x: f\"est={x[0]:.3f}, se={x[1]:.3f}\"\n",
    ")\n",
    "print(final_contrasts_to_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98d4cea",
   "metadata": {},
   "source": [
    "## Part 2: Bayesian Nonlinear Multilevel Analysis\n",
    "\n",
    "This section uses `PyMC` to fit a single nonlinear multilevel model to the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f59f7917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Building and Fitting Bayesian Model with PyMC (this will take several minutes) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [sd_a, sd_b, a_coeffs, b_coeffs, a_pair_offset, b_pair_offset, sigma]\n",
      "Sampling 4 chains for 2_000 tune and 2_000 draw iterations (8_000 + 8_000 draws total) took 22 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Posterior Summary for Contrasts on Alpha ---\n",
      "                 lower  median  upper\n",
      "familiarity      0.745   1.441  2.134\n",
      "duration_linear -0.132   0.427  0.995\n",
      "\n",
      "--- Posterior Summary for Contrasts on Q0 ---\n",
      "                 lower  median  upper\n",
      "familiarity     -2.553  -1.376 -0.209\n",
      "duration_linear -1.256  -0.354  0.560\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Building and Fitting Bayesian Model with PyMC (this will take several minutes) ---\")\n",
    "\n",
    "# Create numeric indices for pair, fmlr, and cond\n",
    "dat_agg['pair_idx'] = pd.Categorical(dat_agg['pair']).codes\n",
    "dat_agg['fmlr_cond_idx'] = pd.Categorical(dat_agg['fmlr'] + \"_\" + dat_agg['cond']).codes\n",
    "\n",
    "# Get dimensions\n",
    "n_pairs = dat_agg['pair_idx'].nunique()\n",
    "n_fmlr_conds = dat_agg['fmlr_cond_idx'].nunique()\n",
    "\n",
    "with pm.Model() as pymc_model:\n",
    "    sd_a = pm.Cauchy(\"sd_a\", 0, 2.5)\n",
    "    sd_b = pm.Cauchy(\"sd_b\", 0, 2.5)\n",
    "    \n",
    "    a_coeffs = pm.Cauchy(\"a_coeffs\", -6, 2.5, shape=n_fmlr_conds)\n",
    "    b_coeffs = pm.Cauchy(\"b_coeffs\", 4, 2.5, shape=n_fmlr_conds)\n",
    "    \n",
    "    a_pair_offset = pm.Normal(\"a_pair_offset\", 0, 1, shape=n_pairs)\n",
    "    a_pair = pm.Deterministic(\"a_pair\", a_pair_offset * sd_a)\n",
    "    \n",
    "    b_pair_offset = pm.Normal(\"b_pair_offset\", 0, 1, shape=n_pairs)\n",
    "    b_pair = pm.Deterministic(\"b_pair\", b_pair_offset * sd_b)\n",
    "\n",
    "    a_est = a_coeffs[dat_agg['fmlr_cond_idx'].values] + a_pair[dat_agg['pair_idx'].values]\n",
    "    b_est = b_coeffs[dat_agg['fmlr_cond_idx'].values] + b_pair[dat_agg['pair_idx'].values]\n",
    "\n",
    "    mu = pt.log10(0.5*pt.exp(b_est) + pt.sqrt(0.25*(pt.exp(b_est)**2) + 1)) * \\\n",
    "         pt.exp((-pt.exp(a_est) / pt.log10(0.5*pt.exp(b_est) + pt.sqrt(0.25*(pt.exp(b_est)**2) + 1))) * pt.exp(b_est) * dat_agg['fr'].values)\n",
    "\n",
    "    sigma = pm.Cauchy(\"sigma\", 0, 2.5)\n",
    "    lq_obs = pm.Normal(\"lq_obs\", mu=mu, sigma=sigma, observed=dat_agg['lq'])\n",
    "    \n",
    "    idata = pm.sample(draws=2000, tune=2000, chains=4, cores=4, target_accept=0.95, progressbar=False)\n",
    "\n",
    "# --- Analyze Posterior ---\n",
    "# Create a mapping from fmlr_cond_idx back to descriptive names\n",
    "idx_to_name = dat_agg[['fmlr_cond_idx', 'fmlr', 'cond']].drop_duplicates().sort_values('fmlr_cond_idx')\n",
    "fmlr_cond_names = (idx_to_name['fmlr'] + \"_\" + idx_to_name['cond']).str.replace(\" \", \"_\").tolist()\n",
    "\n",
    "# Extract the posterior samples into a DataFrame \n",
    "a_samples = idata.posterior['a_coeffs'].stack(sample=(\"chain\", \"draw\")).values.T\n",
    "b_samples = idata.posterior['b_coeffs'].stack(sample=(\"chain\", \"draw\")).values.T\n",
    "\n",
    "a_draws = pd.DataFrame(a_samples, columns=fmlr_cond_names)\n",
    "b_draws = pd.DataFrame(b_samples, columns=fmlr_cond_names)\n",
    "\n",
    "# Programmatically find the column names to make the code robust\n",
    "cagemate_cols_a = [col for col in a_draws.columns if 'Cagemate' in col]\n",
    "non_cagemate_cols_a = [col for col in a_draws.columns if 'Non-cagemate' in col]\n",
    "cagemate_10_a = [col for col in cagemate_cols_a if '10_Sec' in col][0]\n",
    "cagemate_60_a = [col for col in cagemate_cols_a if '60_Sec' in col][0]\n",
    "non_cagemate_10_a = [col for col in non_cagemate_cols_a if '10_Sec' in col][0]\n",
    "non_cagemate_60_a = [col for col in non_cagemate_cols_a if '60_Sec' in col][0]\n",
    "\n",
    "cagemate_cols_b = [col for col in b_draws.columns if 'Cagemate' in col]\n",
    "non_cagemate_cols_b = [col for col in b_draws.columns if 'Non-cagemate' in col]\n",
    "cagemate_10_b = [col for col in cagemate_cols_b if '10_Sec' in col][0]\n",
    "cagemate_60_b = [col for col in cagemate_cols_b if '60_Sec' in col][0]\n",
    "non_cagemate_10_b = [col for col in non_cagemate_cols_b if '10_Sec' in col][0]\n",
    "non_cagemate_60_b = [col for col in non_cagemate_cols_b if '60_Sec' in col][0]\n",
    "\n",
    "# Perform calculations using the retrieved column names\n",
    "alpha_summary = pd.DataFrame({\n",
    "    'familiarity': (a_draws[cagemate_cols_a].sum(axis=1) - a_draws[non_cagemate_cols_a].sum(axis=1)),\n",
    "    'duration_linear': ((-1)*a_draws[cagemate_10_a] + 1*a_draws[cagemate_60_a]) + \\\n",
    "                       ((-1)*a_draws[non_cagemate_10_a] + 1*a_draws[non_cagemate_60_a])\n",
    "})\n",
    "\n",
    "q0_summary = pd.DataFrame({\n",
    "    'familiarity': (b_draws[cagemate_cols_b].sum(axis=1) - b_draws[non_cagemate_cols_b].sum(axis=1)),\n",
    "    'duration_linear': ((-1)*b_draws[cagemate_10_b] + 1*b_draws[cagemate_60_b]) + \\\n",
    "                       ((-1)*b_draws[non_cagemate_10_b] + 1*b_draws[non_cagemate_60_b])\n",
    "})\n",
    "\n",
    "print(\"\\n--- Posterior Summary for Contrasts on Alpha ---\")\n",
    "print(alpha_summary.quantile([0.025, 0.5, 0.975]).T.rename(columns={0.5: 'median', 0.025: 'lower', 0.975: 'upper'}))\n",
    "\n",
    "print(\"\\n--- Posterior Summary for Contrasts on Q0 ---\")\n",
    "print(q0_summary.quantile([0.025, 0.5, 0.975]).T.rename(columns={0.5: 'median', 0.025: 'lower', 0.975: 'upper'}))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
