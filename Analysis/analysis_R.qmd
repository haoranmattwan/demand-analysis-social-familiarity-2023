---
title: "A Behavioral-Economic Demand Analysis of Social Reinforcement"
subtitle: "Replication of Schulingkamp et al. (2023), Frontiers in Psychology"
author: "Haoran (Matt) Wan"
date: "today"
format: 
  html:
    toc: true
    code-fold: false
    self-contained: true
    theme: cosmo
    mainfont: "Garamond"
execute:
  warning: false
  message: false
engine: knitr
---

## Project Objective

This document provides the complete R code to replicate the analyses from the publication:

> Schulingkamp, R., Wan, H., & Hackenberg, T. D. (2023). Social familiarity and reinforcement value: a behavioral-economic analysis of demand for social interaction with cagemate and non-cagemate female rats. *Frontiers in Psychology*, *14*, 1158365. https://doi.org/10.3389/fpsyg.2023.1158365

The study's goal is to quantify the reinforcing value of social interaction in rats using a behavioral-economic demand analysis. Specifically, it tests how this value is affected by **social familiarity** (cagemate vs. non-cagemate) and **reinforcer magnitude** (duration of interaction).

This document demonstrates two distinct analytical approaches to fitting the **Zero-Bounded Exponential (ZBEn) demand model** to the data:
1.  A **frequentist, individual-level analysis** using nonlinear least-squares (`nlsLM`).
2.  A **Bayesian multilevel analysis** using a custom nonlinear model in `brms`, which is better suited for handling repeated measures from a small sample.

The data for this study is available in the Supplementary Material of the original publication at the [publisher's website](https://www.frontiersin.org/articles/10.3389/fpsyg.2023.1158365/full#supplementary-material).

## Analysis Workflow

1.  **Setup & Data Processing**: Loads R packages and the raw dataset, then processes it into an analysis-ready format. This includes calculating response rates and creating dummy variables for the models.
2.  **Frequentist Analysis (Individual-Level)**: Fits the ZBEn demand model to each rat and condition separately using `nlsLM`. Extracts the key model parameters ($Q_{0}$ and $\alpha$) and uses linear contrasts to test for systematic differences across conditions.
3.  **Bayesian Analysis (Multilevel)**: Fits a single, comprehensive Bayesian nonlinear mixed-effects model using `brms`. This approach models all data simultaneously, treating individual rats as random effects to produce more robust parameter estimates.
4.  **Parameter Comparison**: Compares the parameter estimates derived from both the frequentist and Bayesian approaches.

```{r setup_and_process}
# --- 1. SETUP: LOAD PACKAGES AND PROCESS DATA ---

# This section prepares the R environment, loads the raw dataset, and transforms
# it into an analysis-ready format for both the frequentist and Bayesian models.

# --- 1.1 Load R Packages ---
library(minpack.lm) # For robust non-linear least squares regression (nlsLM)
library(multcomp)   # For linear hypothesis testing (contrast tests)
library(brms)       # For Bayesian Regression Models using Stan
library(tidybayes)  # For tidying and visualizing Bayesian model output
library(readr)      # For fast CSV file reading
library(dplyr)      # For data manipulation and wrangling
library(tidyr)      # For tidying data


# --- 1.2 Helper Functions for ZBEn Model ---

# These functions are used for the transformations required by the Zero-Bounded
# Exponential (ZBEn) demand model (Gilroy et al., 2021).

#' @title Inverse Hyperbolic Sine (IHS) Transformation
#' @description A log-like transformation that is defined at zero.
transform_IHS <- function(x) { log10(0.5 * x + sqrt(0.25 * (x^2) + 1)) }

#' @title Anti-Log Transformation for IHS
#' @description The inverse of the IHS transformation.
inverse_IHS <- function(y) { (10^(2 * y) - 1) / (10^y) }

#' @title Essential Value Calculation
#' @description Calculates the essential value (EV) parameter from elasticity.
calculate_EV <- function(alpha) { 1 / (100 * alpha) }


# --- 1.3 Load and Process Data ---

raw_demand_data <- read_csv("dat.csv")

# Process the raw data into an analysis-ready format
processed_demand_data <- raw_demand_data |>
  mutate(
    # Convert active session time from seconds to hours for rate calculation
    active_session_hours = (active_ses_time / 60) / 60,
    # Calculate the primary dependent variable: interaction rate
    interaction_rate = interact / active_session_hours,
    # Apply the IHS transform to the interaction rate for the ZBEn model
    lq = transform_IHS(interaction_rate),
    # Create clear factor labels for familiarity and condition
    familiarity_label = if_else(familiarity == 1, "Cagemate", "Non_cagemate"),
    duration_label = case_when(
      cond == "10sec" ~ "10_Sec",
      cond == "30sec" ~ "30_Sec",
      cond == "60sec" ~ "60_Sec"
    )
  ) |>
  # Average data points at the same price (fr) within each experimental condition
  group_by(pair, familiarity_label, duration_label, fr) |>
  summarise(
    interaction_rate = mean(interaction_rate),
    lq = mean(lq),
    .groups = 'drop'
  ) |>
  # Create a unique ID for each demand curve (rat x familiarity x duration)
  mutate(curve_id = group_indices(across(c(pair, familiarity_label, duration_label))))

# Create dummy variables for use as predictors in the regression models
model_ready_data <- processed_demand_data |>
  # Create a single interaction term for easier dummy coding
  tidyr::unite("condition", familiarity_label:duration_label, remove = FALSE) |>
  fastDummies::dummy_cols(select_columns = c("familiarity_label", "condition")) |>
  # Rename dummy variables to be descriptive and valid R variable names
  rename(
    is_cagemate = `familiarity_label_Cagemate`,
    is_non_cagemate = `familiarity_label_Non_cagemate`,
    cagemate_10s = `condition_Cagemate_10_Sec`,
    cagemate_30s = `condition_Cagemate_30_Sec`,
    cagemate_60s = `condition_Cagemate_60_Sec`,
    non_cagemate_10s = `condition_Non_cagemate_10_Sec`,
    non_cagemate_30s = `condition_Non_cagemate_30_Sec`,
    non_cagemate_60s = `condition_Non_cagemate_60_Sec`
  )
```

---

## 2. Frequentist Analysis (Individual-Level)

This section replicates the first analytical approach from the paper, fitting the **Zero-Bounded Exponential (ZBEn) demand model** to each rat's data separately for all experimental conditions. This individual-level method allows for a direct examination of between-subject variability.

The ZBEn model is a nonlinear function that describes how consumption of a reinforcer (here, social interaction rate) decreases as its price increases. By fitting this model, we can extract two key parameters that quantify the value of the reinforcer:
- **$Q_{0}$ (Demand Intensity)**: The predicted level of consumption when the price is zero. A higher $Q_{0}$ indicates a higher overall demand.
- **$\alpha$ (Elasticity)**: The rate at which consumption decreases as the price increases. A higher $\alpha$ indicates that demand is more sensitive to price (i.e., more elastic).

After fitting the models, we perform linear contrasts on the estimated parameters to test the primary hypotheses: whether social familiarity or interaction duration systematically affected demand intensity ($Q_{0}$) or elasticity ($\alpha$).

```{r frequentist_analysis}
# --- 2.1 Fit ZBEn Model for Each Individual Rat ---

# We will loop through each of the 4 rats, fitting a single complex nonlinear
# model to each one's complete dataset. This model uses dummy variables to
# simultaneously estimate a unique Q0 and alpha parameter for each of the 6
# experimental conditions (2 familiarity x 3 duration).

# Initialize lists to store the results from each rat
individual_parameter_list <- list()
individual_contrast_list <- list()

for (rat_id in 1:4) {
  
  # Filter data for the current rat
  rat_data <- filter(model_ready_data, pair == rat_id)
  
  # Fit the Zero-Bounded Exponential (ZBEn) model using nlsLM
  model_fit <- nlsLM(
    # The formula specifies the ZBEn model in its transformed (IHS) space.
    # It estimates separate 'q' (Q0) and 'a' (alpha) parameters for each
    # of the 6 conditions using the dummy variables created earlier.
    lq ~ transform_IHS(
      cagemate_10s * q_c10 + cagemate_30s * q_c30 + cagemate_60s * q_c60 +
      non_cagemate_10s * q_nc10 + non_cagemate_30s * q_nc30 + non_cagemate_60s * q_nc60
    ) * exp(
      (-exp(
        cagemate_10s * a_c10 + cagemate_30s * a_c30 + cagemate_60s * a_c60 +
        non_cagemate_10s * a_nc10 + non_cagemate_30s * a_nc30 + non_cagemate_60s * a_nc60
      ) / transform_IHS(
        cagemate_10s * q_c10 + cagemate_30s * q_c30 + cagemate_60s * q_c60 +
        non_cagemate_10s * q_nc10 + non_cagemate_30s * q_nc30 + non_cagemate_60s * q_nc60
      )) * (
        cagemate_10s * q_c10 + cagemate_30s * q_c30 + cagemate_60s * q_c60 +
        non_cagemate_10s * q_nc10 + non_cagemate_30s * q_nc30 + non_cagemate_60s * q_nc60
      ) * fr
    ),
    data = rat_data,
    start = list(
      a_c10=-6, a_c30=-6, a_c60=-6, a_nc10=-6, a_nc30=-6, a_nc60=-6,
      q_c10=50, q_c30=50, q_c60=50, q_nc10=50, q_nc30=50, q_nc60=50
    ),
    control = list(maxiter = 1024)
  )
  
  # Store the tidy parameter estimates
  params <- broom::tidy(model_fit)
  params$rat_id <- rat_id
  individual_parameter_list[[rat_id]] <- params
  
  # --- 2.2 Perform Linear Contrasts on Model Parameters ---
  
  # Define a contrast matrix to test the main effect of familiarity on alpha and Q0.
  # This averages across the three duration conditions.
  contrast_matrix <- matrix(c(
    # Row 1: (Cagemate alphas) - (Non-cagemate alphas)
    1, 1, 1, -1, -1, -1, 0, 0, 0, 0, 0, 0,
    # Row 2: (Cagemate Q0s) - (Non-cagemate Q0s)
    0, 0, 0, 0, 0, 0, 1, 1, 1, -1, -1, -1
  ), nrow = 2, byrow = TRUE)
  rownames(contrast_matrix) <- c("alpha", "Q0")
  
  # Execute the general linear hypothesis test and store tidy results
  contrasts <- broom::tidy(summary(glht(model_fit, linfct = contrast_matrix)))
  contrasts$rat_id <- rat_id
  individual_contrast_list[[rat_id]] <- contrasts
}

# --- 2.3 Format and Display Results ---

# Combine the parameter estimates from all rats into a single dataframe
individual_parameter_estimates <- bind_rows(individual_parameter_list) |>
  mutate(
    parameter = if_else(grepl("^a_", term), "alpha", "Q0"),
    Familiarity = if_else(grepl("_c", term), "Cagemate", "Non-cagemate"),
    Duration = case_when(
      grepl("10$", term) ~ "10 Sec",
      grepl("30$", term) ~ "30 Sec",
      grepl("60$", term) ~ "60 Sec"
    )
  ) |>
  select(rat_id, Familiarity, Duration, parameter, estimate, std.error)

# Combine the contrast results from all rats
individual_contrast_results <- bind_rows(individual_contrast_list) |>
  rename(parameter = contrast) |>
  mutate(contrast = "Cagemate vs. Non-cagemate") |>
  select(rat_id, parameter, contrast, estimate, std.error, adj.p.value)

# Display the formatted results tables
cat("--- Estimated ZBEn Parameters (Individual Level) ---\n")
print(individual_parameter_estimates, n = 48)

cat("\n--- Linear Contrast Results ---\n")
print(individual_contrast_results)
```

---

## 3. Bayesian Analysis (Multilevel)

This section presents a more robust and statistically powerful alternative to the individual-level analysis. We fit a single **Bayesian nonlinear multilevel model** to the entire dataset using the `brms` package.

This approach offers several key advantages for this experimental design:
-   **Simultaneous Analysis**: It analyzes all data at once, borrowing strength across individuals and conditions.
-   **Accounts for Repeated Measures**: It formally models the non-independence of the data by treating each rat as a random effect. This is crucial for a repeated-measures design.
-   **Improved Parameter Estimation**: By partially pooling information, it produces more stable and reliable parameter estimates, which is especially important given the small sample size (N=4 rats).
-   **Direct Probabilistic Inference**: It allows us to directly calculate the posterior distributions for our hypotheses (e.g., the effect of familiarity on demand), providing richer, more intuitive results than p-values alone.

```{r bayesian_analysis}
# --- 3.1 Define and Fit the Bayesian Multilevel Model ---

# Define the Stan code for our custom IHS transformation function.
# This code will be inserted into the 'functions' block of the Stan program.
stan_function_code <- "
  real transform_IHS(real x) {
    return log10(0.5 * x + sqrt(0.25 * (x^2) + 1));
  }
"

# Create a 'stanvar' object to pass the function to brms.
custom_stan_functions <- stanvar(scode = stan_function_code, block = "functions")


# Define the brms model formula. It remains the same, but now Stan will
# know what 'transform_IHS' means when it compiles the model.
brms_demand_formula <- bf(
  lq ~ transform_IHS(exp(b)) * exp((-exp(a) / transform_IHS(exp(b))) * exp(b) * fr),
  a ~ 0 + cagemate_10s + cagemate_30s + cagemate_60s + 
          non_cagemate_10s + non_cagemate_30s + non_cagemate_60s + (1 | pair),
  b ~ 0 + cagemate_10s + cagemate_30s + cagemate_60s + 
          non_cagemate_10s + non_cagemate_30s + non_cagemate_60s + (1 | pair),
  nl = TRUE
)

# Priors remain the same.
brms_priors <- c(
  prior(cauchy(0, 2.5), class = "sigma"),
  prior(cauchy(-6, 2.5), class = "b", nlpar = "a"),
  prior(cauchy(0, 2.5), class = "sd", nlpar = "a"),
  prior(cauchy(4, 2.5), class = "b", nlpar = "b"),
  prior(cauchy(0, 2.5), class = "sd", nlpar = "b")
)

# Fit the Bayesian model, now including the 'stanvars' argument.
brms_fit <- brm(
  formula = brms_demand_formula, 
  data = model_ready_data, 
  prior = brms_priors,
  stanvars = custom_stan_functions, # <<< FIX IS HERE
  iter = 4000, warmup = 2000, chains = 4, cores = 4,
  backend = "cmdstanr", 
  control = list(adapt_delta = 0.95, max_treedepth = 10),
  silent = 2, refresh = 0
)

# --- 3.2 Analyze the Posterior Distributions for Key Contrasts ---

# The posterior analysis code remains the same.

# --- Analysis of the Alpha (Elasticity) Parameter ---
cat("--- Posterior Summary for Contrasts on Alpha ---\n")
posterior_alpha_contrasts <- brms_fit |>
  spread_draws(`b_a_.*`, regex = TRUE) |>
  mutate(
    familiarity_effect = (b_a_cagemate_10s + b_a_cagemate_30s + b_a_cagemate_60s) - 
                         (b_a_non_cagemate_10s + b_a_non_cagemate_30s + b_a_non_cagemate_60s),
    duration_linear_trend = ((-1)*b_a_cagemate_10s + 1*b_a_cagemate_60s) + 
                            ((-1)*b_a_non_cagemate_10s + 1*b_a_non_cagemate_60s)
  ) |>
  select(familiarity_effect, duration_linear_trend) |>
  pivot_longer(everything(), names_to = "contrast", values_to = "value") |>
  group_by(contrast) |>
  median_qi()

print(posterior_alpha_contrasts)


# --- Analysis of the Q0 (Demand Intensity) Parameter ---
cat("\n--- Posterior Summary for Contrasts on Q0 ---\n")
posterior_q0_contrasts <- brms_fit |>
  spread_draws(`b_b_.*`, regex = TRUE) |>
  mutate(
    familiarity_effect = (b_b_cagemate_10s + b_b_cagemate_30s + b_b_cagemate_60s) - 
                         (b_b_non_cagemate_10s + b_b_non_cagemate_30s + b_b_non_cagemate_60s),
    duration_linear_trend = ((-1)*b_b_cagemate_10s + 1*b_b_cagemate_60s) + 
                            ((-1)*b_b_non_cagemate_10s + 1*b_b_non_cagemate_60s)
  ) |>
  select(familiarity_effect, duration_linear_trend) |>
  pivot_longer(everything(), names_to = "contrast", values_to = "value") |>
  group_by(contrast) |>
  median_qi()
  
print(posterior_q0_contrasts)
```